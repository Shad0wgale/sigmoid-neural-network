{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "You are presented with a problem of classifying cakes as chocolate or not\n",
    "chocolate based on certain features of the cake. You are given 3 features,\n",
    "- X1: Sugar Content (Values ranging from 0-1)\n",
    "- X2: Cocoa Content (Values ranging from 0-1)\n",
    "- X3: Flour Content (Values ranging from 0-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1:Questions\n",
    "Answer the following questions in short sentences:\n",
    "- How well did the following MLP perform?\n",
    "- Why might the MLP perform as it did, despite the lack of weight updates?\n",
    "- How would the performance change if backpropagation were introduced?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The high mean squared error indicated poor performance from the MLP, meaning that the predicted outputs were very different from the true values.\n",
    "2. Without weight updating, our multilayer perceptron might as well be a linear model because it is not intelligently learning from its iterations at all.\n",
    "3. With back propogation, our MLP would work as intended and adjust/learn by adjusting weights based on output. This will definitely reduce our MSE and improve prediction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Coding\n",
    "Using only the numpy library, create the same multi-layered perceptron in python\n",
    "and initialize using the same parameters and print the MSE. Make sure to\n",
    "structure/generalize your code (either using functions and or classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first weighting: 1.788 2.0940000000000003\n",
      "first sigmoid: 0.8566818955968716 0.8903186413438859\n",
      "second weighting: 1.895164315848643\n",
      "final prediction: 0.8693432420162845 \n",
      "\n",
      "first weighting: 1.421 2.1670000000000003\n",
      "first sigmoid: 0.8054951369940119 0.8972467108359328\n",
      "second weighting: 1.8645255891637622\n",
      "final prediction: 0.8658235707990888 \n",
      "\n",
      "first weighting: 1.815 2.198\n",
      "first sigmoid: 0.8599650843722273 0.9000697663968664\n",
      "second weighting: 1.901003816072757\n",
      "final prediction: 0.8700050956219487 \n",
      "\n",
      " predicted y values: (np.float64(0.8693432420162845), np.float64(0.8658235707990888))\n",
      " mean squared error: 0.2612067731074529\n"
     ]
    }
   ],
   "source": [
    "#input weights\n",
    "w11, w12, w13 = 0.14, 0.78, 0.33\n",
    "w21, w22, w23 = 0.91, 0.47, 0.56\n",
    "\n",
    "#output weights\n",
    "wh1, wh2 = 0.65, 0.38\n",
    "\n",
    "#biases\n",
    "b1, b2, b10 = 1, 1, 1\n",
    "\n",
    "#inputs\n",
    "X = np.array([\n",
    "    [0.4, 0.6, 0.8],\n",
    "    [0.8, 0.1, 0.7],\n",
    "    [0.55, 0.65, 0.7]\n",
    "])\n",
    "\n",
    "#true output\n",
    "y_true = np.array([1, 0, 1])\n",
    "\n",
    "n = len(X)\n",
    "\n",
    "# activation formula\n",
    "def neuron_weighting(x1, x2, x3):\n",
    "    z_h1 = w11 * x1 + w12 * x2 + w13 * x3 + b1 #first layer\n",
    "    z_h2 = w21 * x1 + w22 * x2 + w23 * x3 + b2 #second layer\n",
    "    return z_h1, z_h2\n",
    "\n",
    "def sigmoid(x):\n",
    "  postsig = 1 / (1 + math.exp(-x))\n",
    "  return postsig\n",
    "\n",
    "# Output neuron activation\n",
    "def second_weight(z_h1, z_h2):\n",
    "    z_out = wh1 * z_h1 + wh2 * z_h2 + b10\n",
    "    return z_out\n",
    "\n",
    "\n",
    "predictions = np.zeros(n)\n",
    "for i in range(n):\n",
    "    z_h1, z_h2 = neuron_weighting(X[i][0], X[i][1], X[i][2])\n",
    "    print(\"first weighting:\", z_h1, z_h2)\n",
    "    z_h1 = sigmoid(z_h1)\n",
    "    z_h2 = sigmoid(z_h2)\n",
    "    print(\"first sigmoid:\",z_h1,z_h2)\n",
    "    secondweight = second_weight(z_h1,z_h2)\n",
    "    print(\"second weighting:\",secondweight )\n",
    "    prediction = sigmoid(secondweight)\n",
    "    print(\"final prediction:\",prediction,\"\\n\")\n",
    "    predictions[i] = prediction\n",
    "\n",
    "# Calculate MSE\n",
    "mse = np.mean((y_true - predictions) ** 2)\n",
    "print(f\" predicted y values: {predictions[0],predictions[1]}\")\n",
    "print(f\" mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Hands-On\n",
    "Let’s say we’re given the same cake\n",
    "problem and only want to classify\n",
    "using sugar and cocoa content and\n",
    "instead, we will be using another\n",
    "neural network, with 3 hidden neurons\n",
    "and 2 output neurons all using sigmoid\n",
    "as the activation function. If y1 and y2\n",
    "output (0.3,0.7) then it’s classified as\n",
    "chocolate cake, but if the output is\n",
    "(0.7,0.3) then it is classified as not\n",
    "chocolate cake. Given a set of\n",
    "weights, biases, and inputs, manually\n",
    "calculate their outputs and the MSE.\n",
    "Additionally, update the weights and\n",
    "biases twice using backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Coding Portion\n",
    "Using only the numpy library, create the same simple neural network in python\n",
    "and initialize using the same parameters. Update its weights and biases 100 times\n",
    "and print out its new weights and MSE. Make sure to structure/generalize your\n",
    "code (either using functions and or classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first weighting: 1.351 1.222 1.432\n",
      "first sigmoid: 0.7942930678747973 0.772415320751363 0.8072127476700125\n",
      "second weighting: 2.1570095620594794 2.1598037046369134\n",
      "not chocolate, chocolate 0.8963219796428813 0.1036780203571187 \n",
      "\n",
      "first weighting: 1.070527156843683 1.062017620055242 1.1090980576570848\n",
      "first sigmoid: 0.7446971535555402 0.7430759262812286 0.7519609232711787\n",
      "second weighting: 2.121009410813266 2.111770975716224\n",
      "not chocolate, chocolate 0.8929284747530142 0.10707152524698582 \n",
      "\n",
      "first weighting: 1.426881585479934 1.3227442258893287 1.6139325220689993\n",
      "first sigmoid: 0.8064149656007725 0.7896379118756836 0.8339566497675025\n",
      "second weighting: 1.966260415264545 2.0446678620403906\n",
      "not chocolate, chocolate 0.8772088774200262 0.12279112257997382 \n",
      "\n",
      "first weighting: 1.3127778676418083 1.2507305321945918 1.4699615728864894\n",
      "first sigmoid: 0.7879776223958548 0.7774262941766523 0.8130515452297649\n",
      "second weighting: 1.983859275177064 2.048281602293639\n",
      "not chocolate, chocolate 0.8790919622033021 0.1209080377966979 \n",
      "\n",
      "first weighting: 1.2732252012201837 1.1561128620660668 1.3220150655370473\n",
      "first sigmoid: 0.7812943492492304 0.7606256842829028 0.789516765557238\n",
      "second weighting: 2.006510439189848 2.0586888993622567\n",
      "not chocolate, chocolate 0.8814789394813521 0.11852106051864786 \n",
      "\n",
      "Epoch 1/5, Loss: 0.32295438541347654\n",
      "first weighting: 1.3241347665450731 1.2047759248511787 1.4304833390768765\n",
      "first sigmoid: 0.789868801388435 0.7693733041772356 0.8069766144221056\n",
      "second weighting: 1.7570269154212579 1.9015443599819837\n",
      "not chocolate, chocolate 0.8528369111997424 0.14716308880025764 \n",
      "\n",
      "first weighting: 1.047794008203374 1.0479812656450693 1.1074759334117528\n",
      "first sigmoid: 0.7403510632598163 0.7403870583935429 0.7516582480025723\n",
      "second weighting: 1.7598775908987467 1.8834926850392617\n",
      "not chocolate, chocolate 0.8531943286605342 0.14680567133946576 \n",
      "\n",
      "first weighting: 1.3986026209622873 1.3048238192556045 1.6139483865913224\n",
      "first sigmoid: 0.8019620519713954 0.7866457045160735 0.8339588465631665\n",
      "second weighting: 1.526599087818814 1.7593894309065434\n",
      "not chocolate, chocolate 0.8215081751369568 0.17849182486304316 \n",
      "\n",
      "first weighting: 1.2876227014667707 1.234918653742639 1.4693738566532266\n",
      "first sigmoid: 0.78374453493215 0.7746782940937226 0.8129621966724265\n",
      "second weighting: 1.5880206675822104 1.7938674236396133\n",
      "not chocolate, chocolate 0.8303374426009809 0.16966255739901914 \n",
      "\n",
      "first weighting: 1.249919559444462 1.1418157347899331 1.320997550889012\n",
      "first sigmoid: 0.7772859362188546 0.75801285410111 0.789347625117748\n",
      "second weighting: 1.6470681153876097 1.8296213631778213\n",
      "not chocolate, chocolate 0.8384944040252054 0.16150559597479464 \n",
      "\n",
      "Epoch 2/5, Loss: 0.30266302353428626\n",
      "first weighting: 1.298431784114913 1.1897375879957852 1.4317262359780458\n",
      "first sigmoid: 0.7855709365721305 0.7666941287652692 0.8071701408400819\n",
      "second weighting: 1.3342187701493877 1.6305647553959342\n",
      "not chocolate, chocolate 0.7915376126169714 0.20846238738302858 \n",
      "\n",
      "first weighting: 1.025977745753289 1.0355104125602639 1.107830385050466\n",
      "first sigmoid: 0.73613535519739 0.7379828086870244 0.7517244069236905\n",
      "second weighting: 1.395992788458245 1.6554812931270786\n",
      "not chocolate, chocolate 0.8015472345239992 0.19845276547600077 \n",
      "\n",
      "first weighting: 1.3735968819244555 1.290606054155407 1.6168930804488182\n",
      "first sigmoid: 0.7979606623941266 0.7842497524656107 0.8343662018154959\n",
      "second weighting: 1.0870857868351589 1.476411735850642\n",
      "not chocolate, chocolate 0.7478325591507649 0.25216744084923515 \n",
      "\n",
      "first weighting: 1.2658912734855285 1.2223420183402056 1.4706115072100547\n",
      "first sigmoid: 0.7800385904361884 0.772475438516978 0.8131503143222222\n",
      "second weighting: 1.2166081581916437 1.5576055498269012\n",
      "not chocolate, chocolate 0.7714660975179694 0.22853390248203065 \n",
      "\n",
      "first weighting: 1.2301042520442798 1.1302841253527587 1.3212418788094065\n",
      "first sigmoid: 0.7738368202946194 0.7558913296106508 0.7893882485918211\n",
      "second weighting: 1.3266347560804816 1.6279554618160716\n",
      "not chocolate, chocolate 0.7902834391554383 0.2097165608445617 \n",
      "\n",
      "Epoch 3/5, Loss: 0.2852597226275938\n",
      "first weighting: 1.2778361422397557 1.1788395245988013 1.4345737205388764\n",
      "first sigmoid: 0.7820812148454894 0.7647390830473015 0.8076129542474457\n",
      "second weighting: 0.962537514993383 1.39506930123405\n",
      "not chocolate, chocolate 0.7236295701449825 0.2763704298550175 \n",
      "\n",
      "first weighting: 1.0077527355473241 1.0257481966580313 1.1093406108876622\n",
      "first sigmoid: 0.732580125730662 0.736090765142751 0.752006160490258\n",
      "second weighting: 1.094524036822017 1.4693075758535066\n",
      "not chocolate, chocolate 0.7492326695876144 0.25076733041238564 \n",
      "\n",
      "first weighting: 1.3556328284628474 1.2814402933952542 1.6202611355684244\n",
      "first sigmoid: 0.7950490010860428 0.7826948464842944 0.8348311404471199\n",
      "second weighting: 0.7342740909033532 1.2521815793235804\n",
      "not chocolate, chocolate 0.6757424907571487 0.32425750924285135 \n",
      "\n",
      "first weighting: 1.2499858918227682 1.2135577638705575 1.4716369453109595\n",
      "first sigmoid: 0.7772974189721223 0.7709278489203384 0.8133060661604665\n",
      "second weighting: 0.9365137002246415 1.3826001326210233\n",
      "not chocolate, chocolate 0.718394903497087 0.281605096502913 \n",
      "\n",
      "first weighting: 1.2154369169568338 1.1216669741748744 1.3210866013179259\n",
      "first sigmoid: 0.77125953482242 0.7542977921098286 0.78936243185915\n",
      "second weighting: 1.0968939331857193 1.486638588647796\n",
      "not chocolate, chocolate 0.7496776699569658 0.25032233004303417 \n",
      "\n",
      "Epoch 4/5, Loss: 0.2768382358369057\n",
      "first weighting: 1.2637563831038021 1.1715523884573193 1.4362084826309156\n",
      "first sigmoid: 0.7796720704008054 0.7634255026016867 0.807866826492451\n",
      "second weighting: 0.7042440415048827 1.234898090543862\n",
      "not chocolate, chocolate 0.6691280582209128 0.3308719417790872 \n",
      "\n",
      "first weighting: 0.9938580198292758 1.0183050962633464 1.1104699301383378\n",
      "first sigmoid: 0.7298492796509675 0.734642321308918 0.7522167105628192\n",
      "second weighting: 0.8939816318458892 1.3488928789543242\n",
      "not chocolate, chocolate 0.709711157689679 0.29028884231032104 \n",
      "\n",
      "first weighting: 1.3444316591882406 1.2756409310860395 1.6209257703125914\n",
      "first sigmoid: 0.793217781543573 0.7817068529303741 0.8349227652804958\n",
      "second weighting: 0.509725781559258 1.1132261578758478\n",
      "not chocolate, chocolate 0.624742189064867 0.375257810935133 \n",
      "\n",
      "first weighting: 1.2392114427898968 1.2072531658795151 1.4707330726528722\n",
      "first sigmoid: 0.7754267246474018 0.769812567581541 0.8131687838932703\n",
      "second weighting: 0.7636219526225271 1.2783795854465332\n",
      "not chocolate, chocolate 0.6821395818575997 0.3178604181424003 \n",
      "\n",
      "first weighting: 1.2050229462379562 1.1148842931633451 1.3194763195780892\n",
      "first sigmoid: 0.7694171322471582 0.7530385728796263 0.7890945665524569\n",
      "second weighting: 0.9594457616361186 1.4059253999002794\n",
      "not chocolate, chocolate 0.7230108236488624 0.2769891763511376 \n",
      "\n",
      "Epoch 5/5, Loss: 0.2755528980817568\n"
     ]
    }
   ],
   "source": [
    "w11, w12, w21 = 0.21, 0.61, 0.48\n",
    "w22, w31, w32 = 0.13,0.74,0.35\n",
    "wh11,wh12,wh13 = .89,.52,.06\n",
    "wh21,wh22,wh23 = .97,.41,.09\n",
    "b1,b2,b3 = 1,1,1\n",
    "b10,b20 = 1,1\n",
    "\n",
    "X = np.array([\n",
    "    [0.3,0.6,0.3,0.7],\n",
    "    [0.1,0.1,0.7,0.3],\n",
    "    [0.5,0.7,0.3,0.7],\n",
    "    [0.4,0.5,0.3,0.7],\n",
    "    [0.2,0.5,0.7,0.3]\n",
    "])\n",
    "\n",
    "y_true = np.array([1,0,1,1,0])\n",
    "\n",
    "n = len(X)\n",
    "\n",
    "def neuron_weighting(x1, x2):\n",
    "    z_h1 = w11*x1 + w21 * x2 + b1 #first layer\n",
    "    z_h2 = x1*w21 + x2*w22 + b2  #second layer\n",
    "    z_h3 = w31*x1 + w32 *x2 + b3 #third layer\n",
    "    return z_h1, z_h2, z_h3\n",
    "\n",
    "def output_weighting(h1,h2,h3):\n",
    "    output1 = wh11*h1 + wh12 * h2 + wh13 * h3 + b10\n",
    "    output2 = wh21*h1 + wh22*h2  + wh23*h3 + b20\n",
    "    return output1,output2\n",
    "\n",
    "def sigmoid(x):\n",
    "    postsig = 1 / (1 + math.exp(-x))\n",
    "    return postsig\n",
    "\n",
    "predictions = np.zeros(n)\n",
    "\n",
    "def forward(x1,x2):\n",
    "    z_h1, z_h2, z_h3 = neuron_weighting(x1, x2)\n",
    "    print(\"first weighting:\", z_h1, z_h2,z_h3)\n",
    "    z_h1 = sigmoid(z_h1)\n",
    "    z_h2 = sigmoid(z_h2)\n",
    "    z_h3 = sigmoid(z_h3)\n",
    "    print(\"first sigmoid:\",z_h1,z_h2,z_h3)\n",
    "    o1,o2 = output_weighting(z_h1,z_h2,z_h3)\n",
    "    print(\"second weighting:\", o1,o2)\n",
    "    notchocolate = sigmoid(o1)\n",
    "    chocolate = 1-notchocolate\n",
    "    print(\"not chocolate, chocolate\", notchocolate,chocolate,\"\\n\")\n",
    "    return z_h1, z_h2, z_h3, notchocolate\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(n):\n",
    "        x1, x2 = X[i][0], X[i][1]\n",
    "        y = y_true[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        h1, h2, h3, output_not_chocolate = forward(x1, x2)\n",
    "        \n",
    "        # Compute MSE Loss\n",
    "        loss = (output_not_chocolate - y) ** 2\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Compute output layer error\n",
    "        delta_output = output_not_chocolate - y\n",
    "        delta_o1 = delta_output * sigmoid_derivative(output_not_chocolate)\n",
    "        \n",
    "        # Backpropagate to hidden layer\n",
    "        delta_h1 = delta_o1 * wh11 * sigmoid_derivative(h1)\n",
    "        delta_h2 = delta_o1 * wh12 * sigmoid_derivative(h2)\n",
    "        delta_h3 = delta_o1 * wh13 * sigmoid_derivative(h3)\n",
    "        \n",
    "        # Update output layer weights and biases\n",
    "        wh11 -= 1 * delta_o1 * h1\n",
    "        wh12 -= 1 * delta_o1 * h2\n",
    "        wh13 -= 1 * delta_o1 * h3\n",
    "        wh21 -= 1 * delta_o1 * h1\n",
    "        wh22 -= 1 * delta_o1 * h2\n",
    "        wh23 -= 1 * delta_o1 * h3\n",
    "        b10 -= 1 * delta_o1\n",
    "        \n",
    "        # Update hidden layer weights and biases\n",
    "        w11 -= 1 * delta_h1 * x1\n",
    "        w21 -= 1 * delta_h1 * x2\n",
    "        w12 -= 1 * delta_h2 * x1\n",
    "        w22 -= 1 * delta_h2 * x2\n",
    "        w31 -= 1 * delta_h3 * x1\n",
    "        w32 -= 1 * delta_h3 * x2\n",
    "        b1 -= 1 * delta_h1\n",
    "        b2 -= 1 * delta_h2\n",
    "        b3 -= 1 * delta_h3\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/n}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
